---
# generated by https://github.com/hashicorp/terraform-plugin-docs
page_title: "fptcloud_managed_kubernetes_engine_v1 Resource - terraform-provider-fptcloud"
subcategory: ""
description: |-
  Manage managed FKE clusters.
---

# fptcloud_managed_kubernetes_engine_v1

Manage managed FKE clusters.

## Example Usage

### Basic Usage

```hcl
resource "fptcloud_managed_kubernetes_engine_v1" "example" {
  vpc_id       = "your-vpc-id"
  cluster_name = "example-cluster"
  network_id   = "your-network-id"
  k8s_version  = "1.28.0"

  pools {
    name             = "worker-pool-1"
    storage_profile  = "your-storage-profile"
    worker_type      = "your-worker-type"
    worker_disk_size = 40
    scale_min        = 1
    scale_max        = 3
    worker_base      = true
  }
}
```

### GPU Worker Pool

```hcl
resource "fptcloud_managed_kubernetes_engine_v1" "gpu_cluster" {
  vpc_id       = "your-vpc-id"
  cluster_name = "gpu-cluster"
  network_id   = "your-network-id"
  k8s_version  = "1.31.4"

  pools {
    name                  = "gpu-pool"
    storage_profile       = "Premium-SSD"
    worker_type           = "your-gpu-worker-type"
    worker_disk_size      = 40
    scale_min             = 1
    scale_max             = 3
    worker_base           = false
    
    # GPU Configuration
    vgpu_id               = "your-vgpu-id"
    max_client            = 2
    gpu_sharing_client    = "timeSlicing"
    driver_installation_type = "pre-install"
    gpu_driver_version    = "default"
    
    # Required KV labels for GPU pools
    kv {
      name  = "nvidia.com/mig.config"
      value = "all-1g.6gb"  # One of: "all-1g.6gb", "all-2g.12gb", "all-4g.24gb", ...
    }
    kv {
      name  = "worker.fptcloud/type"
      value = "gpu"
    }
  }
}
```

### With Hibernation Schedules

```hcl
resource "fptcloud_managed_kubernetes_engine_v1" "example_with_hibernation" {
  vpc_id       = "your-vpc-id"
  cluster_name = "example-cluster-with-hibernation"
  network_id   = "your-network-id"
  k8s_version  = "1.28.0"

  # Hibernation schedules configuration
  hibernation_schedules = [
    {
      start    = "0 23 * * 2,4"  # Every Tuesday and Thursday at 11 PM
      end      = "0 7 * * 2,4"   # Every Tuesday and Thursday at 7 AM
      location = "Asia/Bangkok"
    },
    {
      start    = "0 22 * * 6"    # Every Saturday at 10 PM
      end      = "0 8 * * 0"     # Every Sunday at 8 AM
      location = "Asia/Bangkok"
    }
  ]

  pools {
    name             = "worker-pool-1"
    storage_profile  = "your-storage-profile"
    worker_type      = "your-worker-type"
    worker_disk_size = 40
    scale_min        = 1
    scale_max        = 3
    worker_base      = true
  }
}
```

### With Auto Upgrade Version

```hcl
resource "fptcloud_managed_kubernetes_engine_v1" "example_with_hibernation" {
  vpc_id       = "your-vpc-id"
  cluster_name = "example-cluster-with-hibernation"
  network_id   = "your-network-id"
  k8s_version  = "1.28.0"

  # Enable auto upgrade verion configuration

  is_enable_auto_upgrade = true
  auto_upgrade_expression = [ "0 0 * * *" ]
  auto_upgrade_timezone = "Asia/Saigon"

  # Disable auto upgrade verion configuration

  is_enable_auto_upgrade = false

  pools {
    name             = "worker-pool-1"
    storage_profile  = "your-storage-profile"
    worker_type      = "your-worker-type"
    worker_disk_size = 40
    scale_min        = 1
    scale_max        = 3
    worker_base      = true
  }
}
```

### KV Labels and Taints Examples

#### KV Labels Example
```hcl
pools {
  # ... other pool configuration ...
  
  # KV labels for node identification and scheduling
  kv {
    name  = "environment"
    value = "production"
  }
  kv {
    name  = "zone"
    value = "us-east-1a"
  }
  kv {
    name  = "instance-type"
    value = "compute-optimized"
  }
}
```

#### Taints Example
```hcl
pools {
  # ... other pool configuration ...
  
  # Taints for workload isolation
  taints {
    key    = "dedicated"
    value  = "gpu-workloads"
    effect = "NoSchedule"
  }
  
  # Taints for spot instances
  taints {
    key    = "spot-instance"
    value  = "true"
    effect = "PreferNoSchedule"
  }
}
```

## Argument Reference

The following arguments are supported:

### Required Arguments

* `vpc_id` - (Required) VPC ID
* `cluster_name` - (Required) Cluster name
* `network_id` - (Required) Subnet ID

### Optional Arguments

* `k8s_version` - (Optional) Kubernetes version
* `purpose` - (Optional) Cluster purpose
* `pod_network` - (Optional) Pod network (subnet ID)
* `pod_prefix` - (Optional) Pod network (prefix)
* `service_network` - (Optional) Service network (subnet ID)
* `service_prefix` - (Optional) Service prefix (prefix)
* `k8s_max_pod` - (Optional) Max pods per node
* `network_type` - (Optional) Network type
* `network_overlay` - (Optional) Whether to encapsulate pod traffic between different subnets or same subnet
* `edge_gateway_id` - (Optional) Edge gateway ID, in the format of urn:vcloud:gateway:<uuid>
* `internal_subnet_lb` - (Optional) Internal subnet for load balancer
* `edge_gateway_name` - (Optional) Edge gateway name
* `is_enable_auto_upgrade` - (Optional) Whether to enable auto-upgrade
* `auto_upgrade_expression` - (Optional) Auto-upgrade cron expressions
* `auto_upgrade_timezone` - (Optional) Timezone for auto-upgrade
* `is_running` - (Optional) Whether the cluster is running
* `hibernation_schedules` - (Optional) List of hibernation schedules for the cluster

### Hibernation Schedules Block

The `hibernation_schedules` block supports the following:

* `start` - (Required) Cron expression for when hibernation should start
* `end` - (Required) Cron expression for when hibernation should end
* `location` - (Required) Timezone for the hibernation schedule (e.g., Asia/Bangkok)

### Cluster Autoscaler Block

The `cluster_autoscaler` block supports the following:

* `is_enable_auto_scaling` - (Optional) Enable cluster autoscaling
* `scale_down_delay_after_add` - (Optional) Delay after adding a node before scale down (seconds)
* `scale_down_delay_after_delete` - (Optional) Delay after deleting a node before scale down (seconds)
* `scale_down_delay_after_failure` - (Optional) Delay after scale down failure (seconds)
* `scale_down_unneeded_time` - (Optional) Time a node should be unneeded before scale down (seconds)
* `scale_down_utilization_threshold` - (Optional) Utilization threshold for scale down
* `scan_interval` - (Optional) Interval between autoscaler scans (seconds)
* `expander` - (Optional) Autoscaler expander strategy

### Cluster Endpoint Access Block

The `cluster_endpoint_access` block supports the following:

* `type` - (Required) Type of cluster endpoint access (public, private, or mixed)
* `allow_cidr` - (Optional) Allowed CIDR blocks for cluster endpoint access

### Pools Block

The `pools` block supports the following:

#### GPU Configuration
The following GPU-related fields are available for all worker pools, but are only validated when `vgpu_id` is specified:

- **`vgpu_id`**: Virtual GPU ID for GPU-enabled worker nodes
- **`max_client`**: Maximum number of clients that can share the GPU (2-48)
- **`gpu_sharing_client`**: GPU sharing strategy (`""` or `"timeSlicing"`)
- **`driver_installation_type`**: Driver installation method (must be `"pre-install"`)
- **`gpu_driver_version`**: GPU driver version (`"default"` or `"latest"`)

**Note**: All GPU-related fields are only validated when `vgpu_id` is specified. For non-GPU pools, these fields can have any value and are not validated.

#### GPU Requirements
When creating a GPU-enabled worker pool, the following KV labels are **mandatory**:

1. **`nvidia.com/mig.config`**: Specifies the MIG (Multi-Instance GPU) configuration
   - **`"all-1g.6gb"`**: 1GB memory per GPU instance
   - **`"all-2g.12gb"`**: 2GB memory per GPU instance  
   - **`"all-4g.24gb"`**: 4GB memory per GPU instance

2. **`worker.fptcloud/type`**: Must be set to `"gpu"` to identify GPU worker nodes

**Example of required GPU KV labels:**
```hcl
kv {
  name  = "nvidia.com/mig.config"
  value = "all-1g.6gb"  # Choose based on your workload requirements
}
kv {
  name  = "worker.fptcloud/type"
  value = "gpu"
}
```

#### Basic Configuration

* `name` - (Required) Pool name
* `storage_profile` - (Required) Pool storage profile
* `worker_type` - (Required) Worker flavor ID
* `worker_disk_size` - (Required) Worker disk size
* `scale_min` - (Required) Minimum number of nodes for autoscaling
* `scale_max` - (Required) Maximum number of nodes for autoscaling
* `worker_base` - (Required) Whether this is the base worker pool
* `network_name` - (Optional) Subnet name
* `network_id` - (Optional) Subnet ID
* `container_runtime` - (Optional) Container runtime
* `tags` - (Optional) Tags for the worker pool
* `kv` - (Optional) Label for the pool
* `vgpu_id` - (Optional) Virtual GPU ID
* `max_client` - (Optional) Maximum number of clients. Must be between 2 and 48 when `vgpu_id` is set (GPU pools)
* `gpu_sharing_client` - (Optional) GPU sharing client. Must be one of: `""` (empty string) or `"timeSlicing"` when `vgpu_id` is set (GPU pools)
* `driver_installation_type` - (Optional) Driver installation type. Must be `"pre-install"` when `vgpu_id` is set (GPU pools)
* `gpu_driver_version` - (Optional) GPU driver version. Must be one of: `"default"` or `"latest"` when `vgpu_id` is set (GPU pools)
* `is_enable_auto_repair` - (Optional) Whether to enable auto-repair

## Attributes Reference

In addition to all arguments above, the following attributes are exported:

* `id` - The ID of the cluster

## Import

Managed Kubernetes Engine clusters can be imported using the format `vpcId/clusterId`:

```bash
terraform import fptcloud_managed_kubernetes_engine_v1.example vpc-123/cluster-456
```

## Validation Rules

The following validation rules are enforced for worker pools:

### GPU Configuration Validation
The following fields are only validated when `vgpu_id` is set (indicating a GPU pool). For non-GPU pools, these fields are not validated and can have any value:

- **`gpu_sharing_client`**: Must be either an empty string (`""`) or `"timeSlicing"`
- **`max_client`**: Must be between 2 and 48 (inclusive)
- **`driver_installation_type`**: Must be `"pre-install"`
- **`gpu_driver_version`**: Must be either `"default"` or `"latest"`
- **`nvidia.com/mig.config`**: One of: `"all-1g.6gb"`, `"all-2g.12gb"`, `"all-4g.24gb"`, ... (when `vgpu_id` is specified)

### Worker Pool Validation
- **`worker_disk_size`**: Must be at least 40GB
- **`scale_max`**: Must be greater than or equal to `scale_min`
- **`worker_base`**: If set to `true`, taints are not allowed
- **Pool names**: Cannot use reserved name `"worker-new"`
- **Duplicate names**: Pool names must be unique within a cluster

### Taint Validation
- **Taint effects**: Must be one of: `"NoSchedule"`, `"PreferNoSchedule"`, or `"NoExecute"`
- **Base worker pools**: Cannot have taints when `worker_base = true`

### Network Validation
- **Network overlay**: Must be either `"Always"` or `"CrossSubnet"`
- **Network type**: Must be either `"calico"` or `"cilium"`
- **Purpose**: Must be either `"public"` or `"private"`

### Kubernetes Version Validation
- **Supported versions**: `1.32.5`, `1.31.4`, `1.30.8`, `1.29.8`, `1.28.13`
- **Version upgrades**: Can only upgrade by one minor version at a time
- **Version downgrades**: Not allowed

### Cluster Autoscaler Validation
- **Expander strategies**: Must be one of: `"Random"`, `"Least-waste"`, `"Most-pods"`, `"Priority"`

### Cluster Endpoint Access Validation
- **Access types**: Must be one of: `"public"`, `"private"`, or `"mixed"`
- **CIDR format**: Must be valid CIDR notation (e.g., `"10.0.0.0/8"`)

## Notes

- Hibernation schedules use cron expressions for the `start` and `end` fields
- The `location` field should be a valid timezone identifier (e.g., "Asia/Bangkok", "UTC", "America/New_York")
- When hibernation schedules are configured, the cluster will automatically hibernate and wake up according to the specified schedule
- Hibernation schedules can be updated after cluster creation
- The cluster must be in a running state to apply hibernation schedules
- Validation errors will be displayed during `terraform plan` and `terraform apply` operations
- GPU-related fields are available for all pools but are only validated when `vgpu_id` is specified

## Error Handling

### Validation Errors
When validation rules are violated, Terraform will display descriptive error messages:

```hcl
# Example of validation error for max_client
Error: Invalid max_client
  max_client must be between 2 and 48 for pool 'gpu-test', got: 1

# Example of validation error for gpu_sharing_client
Error: Invalid gpu_sharing_client
  gpu_sharing_client 'invalid' in pool 'gpu-test' is not allowed. Must be one of: , timeSlicing
```

### Common Validation Issues
1. **GPU Configuration**: Ensure all GPU fields are properly set when `vgpu_id` is specified
2. **Version Constraints**: Check Kubernetes version compatibility and upgrade restrictions
3. **Network Configuration**: Verify network overlay and type settings match your requirements
4. **Pool Naming**: Avoid reserved names and ensure uniqueness across pools

### Troubleshooting
- Run `terraform plan` to identify validation issues before applying changes
- Check the error messages for specific field requirements
- Ensure all required fields are properly configured
- Verify that immutable fields are not being modified during updates
