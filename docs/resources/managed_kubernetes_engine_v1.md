---
# generated by https://github.com/hashicorp/terraform-plugin-docs
page_title: "fptcloud_managed_kubernetes_engine_v1 Resource - terraform-provider-fptcloud"
subcategory: ""
description: |-
  Manage managed FKE clusters.
---

# fptcloud_managed_kubernetes_engine_v1 (Resource)

Manage managed FKE clusters.

## Example Usage

### Basic Usage

```hcl
resource "fptcloud_managed_kubernetes_engine_v1" "example" {
  vpc_id       = "your-vpc-id"
  cluster_name = "example-cluster"
  network_id   = "your-network-id"
  k8s_version  = "1.28.0"

  pools {
    name             = "worker-pool-1"
    storage_profile  = "your-storage-profile"
    worker_type      = "your-worker-type"
    worker_disk_size = 40
    scale_min        = 1
    scale_max        = 3
  }
}
```

### GPU Worker Pool

```hcl
data "fptcloud_vgpu" "vgpu" {
  vpc_id = data.fptcloud_vpc.vpc.id
  filter {
    key = "name"
    values = ["nvidia_a30"]
  }
}

resource "fptcloud_managed_kubernetes_engine_v1" "gpu_cluster" {
  vpc_id       = "your-vpc-id"
  cluster_name = "gpu-cluster"
  network_id   = "your-network-id"
  k8s_version  = "1.31.4"

  pools {
    name                  = "gpu-pool"
    storage_profile       = "Premium-SSD"
    worker_type           = "your-gpu-worker-type"
    worker_disk_size      = 40
    scale_min             = 1
    scale_max             = 3
    
    # GPU Configuration
    vgpu_id               = data.fptcloud_vgpu.vgpu.vgpus[0].id
    max_client            = 0
    gpu_sharing_client    = ""
    driver_installation_type = "pre-install"
    gpu_driver_version    = "latest"
    
    # Required KV labels for GPU pools
    kv = [
      {
        name  = "nvidia.com/mig.config"
        value = "None with Operator"
      },
      {
        name  = "worker.fptcloud/type"
        value = "gpu"
      }
    ]
  }
}
```

### With Hibernation Schedules

```hcl
resource "fptcloud_managed_kubernetes_engine_v1" "example_with_hibernation" {
  vpc_id       = "your-vpc-id"
  cluster_name = "example-cluster-with-hibernation"
  network_id   = "your-network-id"
  k8s_version  = "1.28.0"

  # Hibernation schedules configuration
  hibernation_schedules = [
    {
      start    = "0 23 * * 2,4"  # Every Tuesday and Thursday at 11 PM
      end      = "0 7 * * 2,4"   # Every Tuesday and Thursday at 7 AM
      location = "Asia/Bangkok"
    },
    {
      start    = "0 22 * * 6"    # Every Saturday at 10 PM
      end      = "0 8 * * 0"     # Every Sunday at 8 AM
      location = "Asia/Bangkok"
    }
  ]

  pools {
    name             = "worker-pool-1"
    storage_profile  = "your-storage-profile"
    worker_type      = "your-worker-type"
    worker_disk_size = 40
    scale_min        = 1
    scale_max        = 3
  }
}
```

### With Auto Upgrade Version

```hcl
resource "fptcloud_managed_kubernetes_engine_v1" "example_with_auto_upgrade" {
  vpc_id       = "your-vpc-id"
  cluster_name = "example-cluster-with-auto-upgrade"
  network_id   = "your-network-id"
  k8s_version  = "1.28.0"

  # Enable auto upgrade version configuration
  is_enable_auto_upgrade = true
  auto_upgrade_expression = [ "0 0 * * *" ]
  auto_upgrade_timezone = "Asia/Saigon"

  pools {
    name             = "worker-pool-1"
    storage_profile  = "your-storage-profile"
    worker_type      = "your-worker-type"
    worker_disk_size = 40
    scale_min        = 1
    scale_max        = 3
  }
}
```

### With Cluster Endpoint Access

```hcl
resource "fptcloud_managed_kubernetes_engine_v1" "example_with_endpoint_access" {
  vpc_id       = "your-vpc-id"
  cluster_name = "example-cluster-endpoint"
  network_id   = "your-network-id"
  k8s_version  = "1.31.4"
  purpose      = "private"

  # Cluster endpoint access configuration
  cluster_endpoint_access = {
    type       = "private" 
    allow_cidr = ["10.0.0.0/16", "172.16.0.0/16"]
  }

  pools {
    name             = "worker-pool-1"
    storage_profile  = "Premium-SSD"
    worker_type      = "your-worker-type"
    worker_disk_size = 40
    scale_min        = 1
    scale_max        = 3
  }
}
```

### KV Labels and Taints Examples
```hcl
pools {
  # ... other pool configuration ...
  
  # KV labels for node identification and scheduling
  kv = [
    {
      name  = "environment"
      value = "production"
    },
    {
      name  = "zone"
      value = "us-east-1a"
    }
  ]
}

pools {
  # ... other pool configuration ...
  
  # Taints for workload isolation
  taints = [
    {
      key    = "dedicated"
      value  = "gpu-workloads"
      effect = "NoSchedule"
    },
    {
      key    = "spot-instance"
      value  = "true"
      effect = "PreferNoSchedule"
    }
  ]
}
```

### Complete Example with All Fields
```hcl
resource "fptcloud_managed_kubernetes_engine_v1" "complete_example" {
  # Required fields
  vpc_id       = data.fptcloud_vpc.vpc.id
  cluster_name = "complete-cluster-example"
  network_id   = data.fptcloud_subnet.subnet.id

  # Optional cluster configuration
  k8s_version     = "1.31.4"
  purpose         = "private"
  network_type    = "calico"
  network_overlay = "CrossSubnet"
  is_running      = true

  # Network configuration
  pod_network     = "100.96.0.0"
  pod_prefix      = "11"
  service_network = "100.64.0.0"
  service_prefix  = "13"
  k8s_max_pod     = 110

  # Edge Gateway configuration (for VMW operation clusters)
  edge_gateway_id     = "urn:vcloud:gateway:12345678-1234-1234-1234-123456789012"
  edge_gateway_name   = "my-edge-gateway"
  internal_subnet_lb  = "10.0.1.0/24"

  # Auto upgrade configuration
  is_enable_auto_upgrade = true
  auto_upgrade_expression = ["0 2 * * 0"]  # Every Sunday at 2 AM
  auto_upgrade_timezone   = "Asia/Ho_Chi_Minh"

  # Hibernation schedules
  hibernation_schedules = [
    {
      start    = "0 23 * * 1-5"  # Weekdays at 11 PM
      end      = "0 7 * * 1-5"   # Weekdays at 7 AM
      location = "Asia/Ho_Chi_Minh"
    },
    {
      start    = "0 22 * * 6"    # Saturday at 10 PM
      end      = "0 8 * * 0"     # Sunday at 8 AM
      location = "Asia/Ho_Chi_Minh"
    }
  ]

  # Cluster autoscaler configuration
  cluster_autoscaler = {
    is_enable_auto_scaling           = true
    scale_down_delay_after_add       = 3600
    scale_down_delay_after_delete    = 0
    scale_down_delay_after_failure   = 180
    scale_down_unneeded_time         = 1800
    scale_down_utilization_threshold = 0.5
    scan_interval                    = 10
    expander                        = "least-waste"
  }

  # Cluster endpoint access configuration
  cluster_endpoint_access = {
    type       = "private"
    allow_cidr = ["10.0.0.0/8", "172.16.0.0/12", "192.168.0.0/16"]
  }

  # Base worker pool
  pools {
    name             = "base-pool"
    storage_profile  = "Premium-SSD"
    worker_type      = data.fptcloud_flavor.cpu_flavor.flavors[0].id
    worker_disk_size = 40
    scale_min        = 1
    scale_max        = 3
    worker_base      = true
    network_id       = data.fptcloud_subnet.subnet.id
    network_name     = data.fptcloud_subnet.subnet.name
    container_runtime = "containerd"
    is_enable_auto_repair = true
    tags = ["tag_ID1", "tag_ID2"]

    # KV labels for base pool
    kv = [
      {
        name  = "environment"
        value = "production"
      },
      {
        name  = "role"
        value = "base"
      }
    ]

    # Note: Base pools cannot have taints (worker_base = true)
  }

  # GPU worker pool
  pools {
    name             = "gpu-pool"
    storage_profile  = "Premium-SSD"
    worker_type      = data.fptcloud_flavor.gpu_flavor.flavors[0].id
    worker_disk_size = 100
    scale_min        = 1
    scale_max        = 5
    worker_base      = false
    container_runtime = "containerd"
    is_enable_auto_repair = true

    # GPU configuration
    vgpu_id               = data.fptcloud_vgpu.vgpu.vgpus[0].id
    max_client            = 4
    gpu_sharing_client    = "timeSlicing"
    driver_installation_type = "pre-install"
    gpu_driver_version    = "default"

    # Required KV labels for GPU pools
    kv = [
      {
        name  = "nvidia.com/mig.config"
        value = "all-1g.6gb"
      },
      {
        name  = "worker.fptcloud/type"
        value = "gpu"
      }
    ]

    # Taints for GPU workloads
    taints = [
      {
        key    = "nvidia.com/gpu"
        value  = "true"
        effect = "NoSchedule"
      },
      {
        key    = "workload-type"
        value  = "gpu-intensive"
        effect = "NoSchedule"
      }
    ]
  }
}
```

## Schema

The following arguments are supported:

### Required Arguments

* `vpc_id` - (Required) VPC ID where the cluster will be created
* `cluster_name` - (Required) Name of the Kubernetes cluster
* `network_id` - (Required) Subnet ID for worker nodes

### Optional Arguments

#### Cluster Configuration
* `k8s_version` - (Optional) Kubernetes version. Default: `"1.31.4"`
* `purpose` - (Optional) Cluster purpose. Must be `"public"` or `"private"`. Default: `"public"`
* `network_type` - (Optional) Container network interface type. Must be `"calico"` or `"cilium"`. Default: `"calico"`
* `network_overlay` - (Optional) Network overlay mode. Must be `"Always"` or `"CrossSubnet"`. Default: `"CrossSubnet"`
* `is_running` - (Optional) Whether the cluster is running (for hibernation control). Default: `true`

#### Network Configuration
* `pod_network` - (Optional) Pod network CIDR. Default: `"100.96.0.0"`
* `pod_prefix` - (Optional) Pod network prefix length. Default: `"11"`
* `service_network` - (Optional) Service network CIDR. Default: `"100.64.0.0"`
* `service_prefix` - (Optional) Service network prefix length. Default: `"13"`
* `k8s_max_pod` - (Optional) Maximum number of pods per node. Default: `110`

#### Edge Gateway Configuration (for VMW operation clusters)
* `edge_gateway_id` - (Require) Edge gateway ID in format `urn:vcloud:gateway:<uuid>`
* `edge_gateway_name` - (Optional) Edge gateway name
* `internal_subnet_lb` - (Optional) Internal subnet for load balancer

#### Auto Upgrade Configuration
* `is_enable_auto_upgrade` - (Optional) Enable automatic Kubernetes version upgrades. Default: `false`
* `auto_upgrade_expression` - (Optional) Cron expressions for auto-upgrade schedule. Default: `[]`
* `auto_upgrade_timezone` - (Optional) Timezone for auto-upgrade schedule. Default: `"Asia/Saigon"`

#### Advanced Configuration
* `hibernation_schedules` - (Optional) List of hibernation schedules for the cluster
* `cluster_autoscaler` - (Optional) Cluster autoscaler configuration block
* `cluster_endpoint_access` - (Optional) Cluster endpoint access configuration

### Hibernation Schedules Block

The `hibernation_schedules` block supports the following:

* `start` - (Required) Cron expression for when hibernation should start
* `end` - (Required) Cron expression for when hibernation should end
* `location` - (Required) Timezone for the hibernation schedule (e.g., Asia/Bangkok)

### Cluster Autoscaler Block

The `cluster_autoscaler` block supports the following:

* `is_enable_auto_scaling` - (Optional) Enable cluster autoscaling. Default: `true`
* `scale_down_delay_after_add` - (Optional) Delay in seconds after adding a node before considering it for scale down. Default: `3600` (1 hour)
* `scale_down_delay_after_delete` - (Optional) Delay in seconds after deleting a node before considering scale down again. Default: `0`
* `scale_down_delay_after_failure` - (Optional) Delay in seconds after scale down failure before retrying. Default: `180` (3 minutes)
* `scale_down_unneeded_time` - (Optional) Time in seconds a node should be unneeded before being eligible for scale down. Default: `1800` (30 minutes)
* `scale_down_utilization_threshold` - (Optional) Node utilization threshold below which a node is considered for scale down (0.0-1.0). Default: `0.5` (50%)
* `scan_interval` - (Optional) Interval in seconds between autoscaler scans to evaluate scaling decisions. Default: `10`
* `expander` - (Optional) Autoscaler expander strategy. Must be one of: `"random"`, `"least-waste"`, `"most-pods"`, `"priority"`. Default: `"least-waste"`

### Cluster Endpoint Access Block

The `cluster_endpoint_access` block supports the following:

* `type` - (Optional) Type of cluster endpoint access. Must be one of: `"public"`, `"private"`, `"mixed"`. Default: `"public"`
* `allow_cidr` - (Optional) List of CIDR blocks allowed to access the cluster endpoint. Default: `["0.0.0.0/0"]`


### Pools Block

The `pools` block supports the following arguments. At least one pool is required:

#### Basic Configuration

**Required:**
* `name` - (Required) Pool name. Cannot be `"worker-new"` (reserved name)
* `storage_profile` - (Required) Storage profile for worker nodes (e.g., `"Premium-SSD"`, `"Standard-SSD"`)
* `worker_type` - (Required) Worker node flavor ID
* `worker_disk_size` - (Required) Worker disk size in GB. Minimum: `40`
* `scale_min` - (Required) Minimum number of nodes for autoscaling. Must be ≥ 1
* `scale_max` - (Required) Maximum number of nodes for autoscaling. Must be ≥ `scale_min`

**Optional:**
* `worker_base` - (Optional) Whether this is the base worker pool (required for cluster operation). Default: `false`
* `network_name` - (Optional) Subnet name. Computed from `network_id` if not specified
* `network_id` - (Optional) Subnet ID. Uses cluster's `network_id` if not specified
* `container_runtime` - (Optional) Container runtime. Default: `"containerd"`
* `is_enable_auto_repair` - (Optional) Enable automatic node repair. Default: `true`
* `tags` - (Optional) List of tag IDs for the worker pool

**GPU Configuration (Optional):**
* `vgpu_id` - (Optional) Virtual GPU ID for GPU-enabled worker nodes
* `max_client` - (Optional) Maximum GPU sharing clients (2-48). Only validated when `vgpu_id` is set. Default: `0`
* `gpu_sharing_client` - (Optional) GPU sharing strategy: `""` or `"timeSlicing"`. Only validated when `vgpu_id` is set. Default: `""`
* `driver_installation_type` - (Optional) Driver installation method: `"pre-install"`. Only validated when `vgpu_id` is set. Default: `""`
* `gpu_driver_version` - (Optional) GPU driver version: `"default"` or `"latest"`. Only validated when `vgpu_id` is set. Default: `""`

**Labels and Taints:**
* `kv` - (Optional) Key-value labels for the pool (list of objects)
  - `name` - (Required) Label key
  - `value` - (Required) Label value
* `taints` - (Optional) Kubernetes taints for the pool (list of objects)
  - `key` - (Required) Taint key
  - `value` - (Required) Taint value
  - `effect` - (Required) Taint effect: `"NoSchedule"`, `"PreferNoSchedule"`, or `"NoExecute"`
  
#### GPU Configuration
The following GPU-related fields are available for all worker pools, but are only validated when `vgpu_id` is specified:

- **`vgpu_id`**: Virtual GPU ID for GPU-enabled worker nodes
- **`max_client`**: Maximum number of clients that can share the GPU (2-48)
- **`gpu_sharing_client`**: GPU sharing strategy (`""` or `"timeSlicing"`)
- **`driver_installation_type`**: Driver installation method (must be `"pre-install"`)
- **`gpu_driver_version`**: GPU driver version (`"default"` or `"latest"`)

**Note**: All GPU-related fields are only validated when `vgpu_id` is specified. For non-GPU pools, these fields can have any value and are not validated.

### Read-Only Arguments
* `id` - The ID of the cluster


## Validation Rules

The following validation rules are enforced for worker pools:

### GPU Configuration Validation
The following fields are only validated when `vgpu_id` is set (indicating a GPU pool). For non-GPU pools, these fields are not validated and can have any value:

- **`gpu_sharing_client`**: Must be either an empty string (`""`) or `"timeSlicing"`
- **`max_client`**: Must be between 2 and 48 (inclusive)
- **`driver_installation_type`**: Must be `"pre-install"`
- **`gpu_driver_version`**: Must be either `"default"` or `"latest"`
- **`nvidia.com/mig.config`**: One of: `"all-1g.6gb"`, `"all-2g.12gb"`, `"all-4g.24gb"`, ... (when `vgpu_id` is specified)

### Worker Pool Validation
- **`worker_disk_size`**: Must be at least 40GB
- **`scale_max`**: Must be greater than or equal to `scale_min`
- **`worker_base`**: If set to `true`, taints are not allowed
- **Duplicate names**: Pool names must be unique within a cluster

### Taint Validation
- **Taint effects**: Must be one of: `"NoSchedule"`, `"PreferNoSchedule"`, or `"NoExecute"`
- **Base worker pools**: Cannot have taints when `worker_base = true`

### Network Validation
- **Network overlay**: Must be either `"Always"` or `"CrossSubnet"`
- **Network type**: Must be either `"calico"` or `"cilium"`
- **Purpose**: Must be either `"public"` or `"private"`

### Kubernetes Version Validation
- **Supported versions**: `1.32.5`, `1.31.4`, `1.30.8`, `1.29.8`, `1.28.13`

### Cluster Autoscaler Validation
- **Expander strategies**: Must be one of: `"random"`, `"least-waste"`, `"most-pods"`, `"priority"` (case-insensitive)
- **Utilization threshold**: Must be between 0.0 and 1.0
- **Scan Interval**: Must be between 1 and 3600

### Cluster Endpoint Access Validation
- **Access types**: Must be one of: `"public"`, `"private"`, or `"mixed"`
- **CIDR format**: Must be valid CIDR notation (e.g., `"10.0.0.0/8"`)

## Error Handling

### Validation Errors
When validation rules are violated, Terraform will display descriptive error messages:

```hcl
# Example of validation error for max_client
Error: Invalid max_client
  max_client must be between 2 and 48 for pool 'gpu-test', got: 1

# Example of validation error for gpu_sharing_client
Error: Invalid gpu_sharing_client
  gpu_sharing_client 'invalid' in pool 'gpu-test' is not allowed. Must be one of: , timeSlicing
```
